# llama.cpp

![llama](https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png)

[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Server](https://github.com/ggerganov/llama.cpp/actions/workflows/server.yml/badge.svg?branch=master&event=schedule)](https://github.com/ggerganov/llama.cpp/actions/workflows/server.yml)
[![Conan Center](https://shields.io/conan/v/llama-cpp)](https://conan.io/center/llama-cpp)

[Roadmap](https://github.com/users/ggerganov/projects/7) / [Project status](https://github.com/ggerganov/llama.cpp/discussions/3471) / [Manifesto](https://github.com/ggerganov/llama.cpp/discussions/205) / [ggml](https://github.com/ggerganov/ggml)

Metaì˜ [LLaMA](https://arxiv.org/abs/2302.13971) ëª¨ë¸(ë° ê¸°íƒ€ ëª¨ë¸)ì„ ìˆœìˆ˜ C/C++ì—ì„œ ì¶”ë¡ í•˜ëŠ” ì†Œí”„íŠ¸ì›¨ì–´

> [!ì¤‘ìš”]
[2024ë…„ 6ì›” 12ì¼] ë°”ì´ë„ˆë¦¬ íŒŒì¼ ì´ë¦„ì´ `llama-` ì ‘ë‘ì‚¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. `main`ì€ `llama-cli`ë¡œ, `server`ëŠ” `llama-server`ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. (https://github.com/ggerganov/llama.cpp/pull/7809)

## ìµœê·¼ API ë³€ê²½ ì‚¬í•­

- [2024ë…„ 6ì›” 26ì¼] ì†ŒìŠ¤ ì½”ë“œ ë° CMake ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ê°€ ì¬êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. https://github.com/ggerganov/llama.cpp/pull/8006
- [2024ë…„ 4ì›” 21ì¼] `llama_token_to_piece`ëŠ” ì´ì œ íŠ¹ìˆ˜ í† í°ì„ ë Œë”ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. https://github.com/ggerganov/llama.cpp/pull/6807
- [2024ë…„ 4ì›” 4ì¼] ìƒíƒœ ë° ì„¸ì…˜ íŒŒì¼ í•¨ìˆ˜ê°€ `llama_state_*` ì•„ë˜ì—ì„œ ì¬ì •ë ¬ë˜ì—ˆìŠµë‹ˆë‹¤. https://github.com/ggerganov/llama.cpp/pull/6341
- [2024ë…„ 3ì›” 26ì¼] ë¡œê·¸ì™€ ì„ë² ë”© APIê°€ ê°„ê²°í•´ì¡ŒìŠµë‹ˆë‹¤. https://github.com/ggerganov/llama.cpp/pull/6122
- [2024ë…„ 3ì›” 13ì¼] `llama_synchronize()` ë° `llama_context_params.n_ubatch` ì¶”ê°€ https://github.com/ggerganov/llama.cpp/pull/6017
- [2024ë…„ 3ì›” 8ì¼] `llama_kv_cache_seq_rm()`ì€ `void` ëŒ€ì‹  `bool`ì„ ë°˜í™˜í•˜ê³ , ìƒˆë¡œìš´ `llama_n_seq_max()`ëŠ” ë°°ì¹˜ì—ì„œ í—ˆìš© ê°€ëŠ¥í•œ `seq_id`ì˜ ìƒí•œì„ ì„ ë°˜í™˜í•©ë‹ˆë‹¤. (ì—¬ëŸ¬ ë¬¸ì¥ì„ ì²˜ë¦¬í•  ë•Œ ê´€ë ¨ë¨) https://github.com/ggerganov/llama.cpp/pull/5328
- [2024ë…„ 3ì›” 4ì¼] ì„ë² ë”© API ì—…ë°ì´íŠ¸ https://github.com/ggerganov/llama.cpp/pull/5796
- [2024ë…„ 3ì›” 3ì¼] `struct llama_context_params` https://github.com/ggerganov/llama.cpp/pull/5849

## ëœ¨ê±°ìš´ ì£¼ì œ

- **`convert.py`ëŠ” `examples/convert_legacy_llama.py`ë¡œ ì´ë™ë˜ì–´ ë¶ˆì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. `convert_hf_to_gguf.py`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”** https://github.com/ggerganov/llama.cpp/pull/7430
- ì´ˆê¸° Flash-Attention ì§€ì›: https://github.com/ggerganov/llama.cpp/pull/5021
- BPE ì‚¬ì „ í† í°í™” ì§€ì›ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤: https://github.com/ggerganov/llama.cpp/pull/6920
- MoE ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤ - `mmap` ì§€ì›ì„ ìœ„í•´ ëª¨ë¸ì„ ë‹¤ì‹œ ë³€í™˜í•˜ê³  `imatrix`ë¥¼ ì¬ìƒì„±í•˜ì„¸ìš” https://github.com/ggerganov/llama.cpp/pull/6387
- `gguf-split`ì„ ì‚¬ìš©í•œ ëª¨ë¸ ì¡°ê°í™” ì§€ì¹¨ https://github.com/ggerganov/llama.cpp/discussions/6404
- Metal ë°°ì¹˜ ì¶”ë¡ ì—ì„œ ì£¼ìš” ë²„ê·¸ ìˆ˜ì • https://github.com/ggerganov/llama.cpp/pull/6225
- ë‹¤ì¤‘ GPU íŒŒì´í”„ë¼ì¸ ë³‘ë ¬ ì§€ì› https://github.com/ggerganov/llama.cpp/pull/6017
- Deepseek ì§€ì› ì¶”ê°€ë¥¼ ìœ„í•œ ê¸°ì—¬ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤: https://github.com/ggerganov/llama.cpp/issues/5981
- ì–‘ìí™” ì‹¤ëª… í…ŒìŠ¤íŠ¸: https://github.com/ggerganov/llama.cpp/discussions/5962
- ì´ˆê¸° Mamba ì§€ì›ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤: https://github.com/ggerganov/llama.cpp/pull/5328

----

## ì„¤ëª…

`llama.cpp`ì˜ ì£¼ìš” ëª©í‘œëŠ” ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ì—ì„œ ìµœì†Œí•œì˜ ì„¤ì •ê³¼ ìµœì²¨ë‹¨ ì„±ëŠ¥ìœ¼ë¡œ LLM ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

- C/C++ë¡œ ì‘ì„±ëœ ìˆœìˆ˜í•œ êµ¬í˜„ìœ¼ë¡œ ì˜ì¡´ì„±ì´ ì—†ìŠµë‹ˆë‹¤.
- Apple siliconì€ ARM NEON, Accelerate ë° Metal í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ìµœì í™”ëœ ìµœìš°ì„  ì§€ì› ëŒ€ìƒì…ë‹ˆë‹¤.
- x86 ì•„í‚¤í…ì²˜ì— ëŒ€í•œ AVX, AVX2 ë° AVX512 ì§€ì›
- ë” ë¹ ë¥¸ ì¶”ë¡ ê³¼ ê¸°ì–µ ì‚¬ìš©ëŸ‰ ê°ì†Œë¥¼ ìœ„í•œ 1.5ë¹„íŠ¸, 2ë¹„íŠ¸, 3ë¹„íŠ¸, 4ë¹„íŠ¸, 5ë¹„íŠ¸, 6ë¹„íŠ¸ ë° 8ë¹„íŠ¸ ì •ìˆ˜ ì–‘ìí™”
- NVIDIA GPUì—ì„œ LLMì„ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì‚¬ìš©ì ì •ì˜ CUDA ì»¤ë„ (HIPì„ í†µí•œ AMD GPU ì§€ì›)
- Vulkan ë° SYCL ë°±ì—”ë“œ ì§€ì›
- ì „ì²´ VRAM ìš©ëŸ‰ë³´ë‹¤ í° ëª¨ë¸ì„ ë¶€ë¶„ì ìœ¼ë¡œ ê°€ì†í™”í•˜ê¸° ìœ„í•œ CPU+GPU í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ 

[ì‹œì‘](https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022) ì´í›„ë¡œ í”„ë¡œì íŠ¸ëŠ” ë§ì€ ê¸°ì—¬ ë•ë¶„ì— í¬ê²Œ ë°œì „í–ˆìŠµë‹ˆë‹¤. `ggml` ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•œ ìƒˆë¡œìš´ ê¸°ëŠ¥ ê°œë°œì˜ ì£¼ìš” í”Œë«í¼ì…ë‹ˆë‹¤.

**ì§€ì›ë˜ëŠ” ëª¨ë¸:**

ì¼ë°˜ì ìœ¼ë¡œ ì•„ë˜ ê¸°ë³¸ ëª¨ë¸ì˜ ë¯¸ì„¸ ì¡°ì •ëœ ë²„ì „ë„ ì§€ì›ë©ë‹ˆë‹¤.

- [X] LLaMA ğŸ¦™
- [x] LLaMA 2 ğŸ¦™ğŸ¦™
- [x] LLaMA 3 ğŸ¦™ğŸ¦™ğŸ¦™
- [X] [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)
- [x] [Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral)
- [x] [DBRX](https://huggingface.co/databricks/dbrx-instruct)
- [X] [Falcon](https://huggingface.co/models?search=tiiuae/falcon)
- [X] [Chinese LLaMA / Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) ë° [Chinese LLaMA-2 / Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)
- [X] [Vigogne (í”„ë‘ìŠ¤ì–´)](https://github.com/bofenghuang/vigogne)
- [X] [BERT](https://github.com/ggerganov/llama.cpp/pull/5423)
- [X] [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)
- [X] [Baichuan 1 & 2](https://huggingface.co/models?search=baichuan-inc/Baichuan) + [íŒŒìƒ](https://huggingface.co/hiyouga/baichuan-7b-sft)
- [X] [Aquila 1 & 2](https://huggingface.co/models?search=BAAI/Aquila)
- [X] [Starcoder ëª¨ë¸](https://github.com/ggerganov/llama.cpp/pull/3187)
- [X] [Refact](https://huggingface.co/smallcloudai/Refact-1_6B-fim)
- [X] [MPT](https://github.com/ggerganov/llama.cpp/pull/3417)
- [X] [Bloom](https://github.com/ggerganov/llama.cpp/pull/3553)
- [x] [Yi ëª¨ë¸](https://huggingface.co/models?search=01-ai/Yi)
- [X] [StableLM ëª¨ë¸](https://huggingface.co/stabilityai)
- [x] [Deepseek ëª¨ë¸](https://huggingface.co/models?search=deepseek-ai/deepseek)
- [x] [Qwen ëª¨ë¸](https://huggingface.co/models?search=Qwen/Qwen)
- [x] [PLaMo-13B](https://github.com/ggerganov/llama.cpp/pull/3557)
- [x] [Phi ëª¨ë¸](https://huggingface.co/models?search=microsoft/phi)
- [x] [GPT-2](https://huggingface.co/gpt2)
- [x] [Orion 14B](https://github.com/ggerganov/llama.cpp/pull/5118)
- [x] [InternLM2](https://huggingface.co/models?search=internlm2)
- [x] [CodeShell](https://github.com/WisdomShell/codeshell)
- [x] [Gemma](https://ai.google.dev/gemma)
- [x] [Mamba](https://github.com/state-spaces/mamba)
- [x] [Grok-1](https://huggingface.co/keyfan/grok-1-hf)
- [x] [Xverse](https://huggingface.co/models?search=xverse)
- [x] [Command-R ëª¨ë¸](https://huggingface.co/models?search=CohereForAI/c4ai-command-r)
- [x] [SEA-LION](https://huggingface.co/models?search=sea-lion)
- [x] [GritLM-7B](https://huggingface.co/GritLM/GritLM-7B) + [GritLM-8x7B](https://huggingface.co/GritLM/GritLM-8x7B)
- [x] [OLMo](https://allenai.org/olmo)
- [x] [GPT-NeoX](https://github.com/EleutherAI/gpt-neox) + [Pythia](https://github.com/EleutherAI/pythia)
- [x] [ChatGLM3-6b](https://huggingface.co/THUDM/chatglm3-6b) + [ChatGLM4-9b](https://huggingface.co/THUDM/glm-4-9b)

(ë” ë§ì€ ëª¨ë¸ì„ ì§€ì›í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì§€ì¹¨: [HOWTO-add-model.md](./docs/development/HOWTO-add-model.md))

**ë‹¤ì¤‘ ëª¨ë‹¬ ëª¨ë¸:**

- [x] [LLaVA 1.5 ëª¨ë¸](https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e), [LLaVA 1.6 ëª¨ë¸](https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2)
- [x] [BakLLaVA](https://huggingface.co/models?search=SkunkworksAI/Bakllava)
- [x] [Obsidian](https://huggingface.co/NousResearch/Obsidian-3B-V0.5)
- [x] [ShareGPT4V](https://huggingface.co/models?search=Lin-Chen/ShareGPT4V)
- [x] [MobileVLM 1.7B/3B ëª¨ë¸](https://huggingface.co/models?search=mobileVLM)
- [x] [Yi-VL](https://huggingface.co/models?search=Yi-VL)
- [x] [Mini CPM](https://huggingface.co/models?search=MiniCPM)
- [x] [Moondream](https://huggingface.co/vikhyatk/moondream2)
- [x] [Bunny](https://github.com/BAAI-DCAI/Bunny)

**ë°”ì¸ë”©:**

- Python: [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- Go: [go-skynet/go-llama.cpp](https://github.com/go-skynet/go-llama.cpp)
- Node.js: [withcatai/node-llama-cpp](https://github.com/withcatai/node-llama-cpp)
- JS/TS (llama.cpp ì„œë²„ í´ë¼ì´ì–¸íŠ¸): [lgrammel/modelfusion](https://modelfusion.dev/integration/model-provider/llamacpp)
- JavaScript/Wasm (ë¸Œë¼ìš°ì €ì—ì„œ ì‘ë™): [tangledgroup/llama-cpp-wasm](https://github.com/tangledgroup/llama-cpp-wasm)
- TypeScript/Wasm (ë” ì¢‹ì€ API, npmì—ì„œ ì‚¬ìš© ê°€ëŠ¥): [ngxson/wllama](https://github.com/ngxson/wllama)
- Ruby: [yoshoku/llama_cpp.rb](https://github.com/yoshoku/llama_cpp.rb)
- Rust (ë” ë§ì€ ê¸°ëŠ¥): [edgenai/llama_cpp-rs](https://github.com/edgenai/llama_cpp-rs)
- Rust (ë” ì¢‹ì€ API): [mdrokz/rust-llama.cpp](https://github.com/mdrokz/rust-llama.cpp)
- Rust (ë” ì§ì ‘ì ì¸ ë°”ì¸ë”©): [utilityai/llama-cpp-rs](https://github.com/utilityai/llama-cpp-rs)
- C#: [SciSharp/LLamaSharp](https://github.com/SciSharp/LLamaSharp)
- Scala 3: [donderom/llm4s](https://github.com/donderom/llm4s)
- Clojure: [phronmophobic/llama.clj](https://github.com/phronmophobic/llama.clj)
- React Native: [mybigday/llama.rn](https://github.com/mybigday/llama.rn)
- Java: [kherud/java-llama.cpp](https://github.com/kherud/java-llama.cpp)
- Zig: [deins/llama.cpp.zig](https://github.com/Deins/llama.cpp.zig)
- Flutter/Dart: [netdur/llama_cpp_dart](https://github.com/netdur/llama_cpp_dart)
- PHP (llama.cppì— ëŒ€í•œ API ë°”ì¸ë”© ë° ìœ„ì—ì„œ êµ¬ì¶•ëœ ê¸°ëŠ¥): [distantmagic/resonance](https://github.com/distantmagic/resonance) [(ë” ìì„¸í•œ ë‚´ìš©)](https://github.com/ggerganov/llama.cpp/pull/6326)
- Guile Scheme: [guile_llama_cpp](https://savannah.nongnu.org/projects/guile-llama-cpp)

**UI:**

ë‹¤ìŒ í”„ë¡œì íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì˜¤í”ˆ ì†ŒìŠ¤ì´ë©° í—ˆìš©í•˜ëŠ” ë¼ì´ì„¼ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

- [iohub/coLLaMA](https://github.com/iohub/coLLaMA)
- [janhq/jan](https://github.com/janhq/jan) (AGPL)
- [nat/openplayground](https://github.com/nat/openplayground)
- [Faraday](https://faraday.dev/) (ì‚¬ìš©ì ì§€ì •)
- [LMStudio](https://lmstudio.ai/) (ì‚¬ìš©ì ì§€ì •)
- [Layla](https://play.google.com/store/apps/details?id=com.laylalite) (ì‚¬ìš©ì ì§€ì •)
- [LocalAI](https://github.com/mudler/LocalAI) (MIT)
- [LostRuins/koboldcpp](https://github.com/LostRuins/koboldcpp) (AGPL)
- [Mozilla-Ocho/llamafile](https://github.com/Mozilla-Ocho/llamafile)
- [nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all)
- [ollama/ollama](https://github.com/ollama/ollama)
- [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) (AGPL)
- [psugihara/freechat](https://github.com/psugihara/freechat)
- [cztomsik/ava](https://github.com/cztomsik/ava) (MIT)
- [ptsochantaris/emeltal](https://github.com/ptsochantaris/emeltal)
- [pythops/tenere](https://github.com/pythops/tenere) (AGPL)
- [RAGNA Desktop](https://ragna.app/) (ì‚¬ìš©ì ì§€ì •)
- [RecurseChat](https://recurse.chat/) (ì‚¬ìš©ì ì§€ì •)
- [semperai/amica](https://github.com/semperai/amica)
- [withcatai/catai](https://github.com/withcatai/catai)
- [Mobile-Artificial-Intelligence/maid](https://github.com/Mobile-Artificial-Intelligence/maid) (MIT)
- [Msty](https://msty.app) (ì‚¬ìš©ì ì§€ì •)
- [LLMFarm](https://github.com/guinmoon/LLMFarm?tab=readme-ov-file)(MIT)
- [KanTV](https://github.com/zhouwg/kantv?tab=readme-ov-file)(Apachev2.0 or later)
- [Dot](https://github.com/alexpinel/Dot) (GPL)
- [MindMac](https://mindmac.app) (ì‚¬ìš©ì ì§€ì •)
- [KodiBot](https://github.com/firatkiral/kodibot) (GPL)
- [eva](https://github.com/ylsdamxssjxxdd/eva) (MIT)
- [AI Sublime Text í”ŒëŸ¬ê·¸ì¸](https://github.com/yaroslavyaroslav/OpenAI-sublime-text) (MIT)
- [AIKit](https://github.com/sozercan/aikit) (MIT)
- [LARS - The LLM & Advanced Referencing Solution](https://github.com/abgulati/LARS) (AGPL)

*(í”„ë¡œì íŠ¸ê°€ `llama.cpp`ì— ì˜ì¡´í•œë‹¤ê³  ëª…ì‹œë˜ì–´ì•¼ë§Œ ì—¬ê¸°ì— í¬í•¨ë©ë‹ˆë‹¤.)

**ë„êµ¬:**

- [akx/ggify](https://github.com/akx/ggify) â€“ HuggingFace Hubì—ì„œ PyTorch ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ GGMLë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- [crashr/gppm](https://github.com/crashr/gppm) â€“ NVIDIA Tesla P40 ë˜ëŠ” P100 GPUë¥¼ ì‚¬ìš©í•˜ì—¬ llama.cpp ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‹¤í–‰í•˜ê³ , ë¹„í™œì„± ìƒíƒœ ì‹œ ì „ë ¥ ì†Œë¹„ëŸ‰ì„ ì¤„ì…ë‹ˆë‹¤.

**ì¸í”„ë¼:**

- [Paddler](https://github.com/distantmagic/paddler) - llama.cppì— ë§ì¶¤í˜• ë¡œë“œ ë°¸ëŸ°ì„œì…ë‹ˆë‹¤.

## ë°ëª¨

<details>
<summary>M2 Ultraì—ì„œ LLaMA v2 13Bë¥¼ ì‚¬ìš©í•œ ì¼ë°˜ì ì¸ ì‹¤í–‰</summary>

```
$ make -j && ./llama-cli -m models/llama-13b-v2/ggml-model-q4_0.gguf -p "Building a website can be done in 10 simple steps:\nStep 1:" -n 400 -e
I llama.cpp build info:
I UNAME_S:  Darwin
I UNAME_P:  arm
I UNAME_M:  arm64
I CFLAGS:   -I.            -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE
I CXXFLAGS: -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS
I LDFLAGS:   -framework Accelerate
I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)
I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)

make: Nothing to be done for `default'.
main: build = 1041 (cf658ad)
main: seed  = 1692823051
llama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from models/llama-13b-v2/ggml-model-q4_0.gguf (version GGUF V1 (latest))
llama_model_loader: - type  f32:   81 tensors
llama_model_loader: - type q4_0:  281 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_print_meta: format         = GGUF V1 (latest)
llm_load_print_meta: arch           = llama
llm_load_print_meta: vocab type     = SPM
llm_load_print_meta: n_vocab        = 32000
llm_load_print_meta: n_merges       = 0
llm_load_print_meta: n_ctx_train    = 4096
llm_load_print_meta: n_ctx          = 512
llm_load_print_meta: n_embd         = 5120
llm_load_print_meta: n_head         = 40
llm_load_print_meta: n_head_kv      = 40
llm_load_print_meta: n_layer        = 40
llm_load_print_meta: n_rot          = 128
llm_load_print_meta: n_gqa          = 1
llm_load_print_meta: f_norm_eps     = 1.0e-05
llm_load_print_meta: f_norm_rms_eps = 1.0e-05
llm_load_print_meta: n_ff           = 13824
llm_load_print_meta: freq_base      = 10000.0
llm_load_print_meta: freq_scale     = 1
llm_load_print_meta: model type     = 13B
llm_load_print_meta: model ftype    = mostly Q4_0
llm_load_print_meta: model size     = 13.02 B
llm_load_print_meta: general.name   = LLaMA v2
llm_load_print_meta: BOS token = 1 '<s>'
llm_load_print_meta: EOS token = 2 '</s>'
llm_load_print_meta: UNK token = 0 '<unk>'
llm_load_print_meta: LF token  = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.11 MB
llm_load_tensors: mem required  = 7024.01 MB (+  400.00 MB per state)
...................................................................................................
llama_new_context_with_model: kv self size  =  400.00 MB
llama_new_context_with_model: compute buffer total size =   75.41 MB

system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = 400, n_keep = 0


 Building a website can be done in 10 simple steps:
Step 1: Find the right website platform.
Step 2: Choose your domain name and hosting plan.
Step 3: Design your website layout.
Step 4: Write your website content and add images.
Step 5: Install security features to protect your site from hackers or spammers
Step 6: Test your website on multiple browsers, mobile devices, operating systems etcâ€¦
Step 7: Test it again with people who are not related to you personally â€“ friends or family members will work just fine!
Step 8: Start marketing and promoting the website via social media channels or paid ads
Step 9: Analyze how many visitors have come to your site so far, what type of people visit more often than others (e.g., men vs women) etcâ€¦
Step 10: Continue to improve upon all aspects mentioned above by following trends in web design and staying up-to-date on new technologies that can enhance user experience even further!
How does a Website Work?
A website works by having pages, which are made of HTML code. This code tells your computer how to display the content on each page you visit â€“ whether itâ€™s an image or text file (like PDFs). In order for someone elseâ€™s browser not only be able but also want those same results when accessing any given URL; some additional steps need taken by way of programming scripts that will add functionality such as making links clickable!
The most common type is called static HTML pages because they remain unchanged over time unless modified manually (either through editing files directly or using an interface such as WordPress). They are usually served up via HTTP protocols â€“ this means anyone can access them without having any special privileges like being part of a group who is allowed into restricted areas online; however, there may still exist some limitations depending upon where one lives geographically speaking.
How to
llama_print_timings:        load time =   576.45 ms
llama_print_timings:      sample time =   283.10 ms /   400 runs   (    0.71 ms per token,  1412.91 tokens per second)
llama_print_timings: prompt eval time =   599.83 ms /    19 tokens (   31.57 ms per token,    31.68 tokens per second)
llama_print_timings:        eval time = 24513.59 ms /   399 runs   (   61.44 ms per token,    16.28 tokens per second)
llama_print_timings:       total time = 25431.49 ms
```

</details>

<details>
<summary>M1 Pro MacBookì—ì„œ LLaMA-7Bì™€ whisper.cppë¥¼ ë™ì‹œì— ì‹¤í–‰í•˜ëŠ” ë°ëª¨</summary>

ë‹¤ìŒì€ M1 Pro MacBookì—ì„œ LLaMA-7Bì™€ [whisper.cpp](https://github.com/ggerganov/whisper.cpp)ë¥¼ ë™ì‹œì— ì‹¤í–‰í•˜ëŠ” ë°ëª¨ì…ë‹ˆë‹¤:

https://user-images.githubusercontent.com/1991296/224442907-7693d4be-acaa-4e01-8b4f-add84093ffff.mp4

</details>

## ì‚¬ìš©ë²•

ë‹¤ìŒì€ ëŒ€ë¶€ë¶„ì˜ ì§€ì› ëª¨ë¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ ì´ì§„ íŒŒì¼ ë¹Œë“œ ë° ëª¨ë¸ ë³€í™˜ ë‹¨ê³„ì…ë‹ˆë‹¤.

### ê¸°ë³¸ ì‚¬ìš©ë²•

ë¨¼ì €, ë°”ì´ë„ˆë¦¬ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼í•  ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.
- ë°©ë²• 1: ì´ ì €ì¥ì†Œë¥¼ ë³µì œí•˜ê³  ë¡œì»¬ë¡œ ë¹Œë“œí•©ë‹ˆë‹¤. [ë¹Œë“œ ë°©ë²•](./docs/build.md)ì„ ì°¸ì¡°í•˜ì„¸ìš”.
- ë°©ë²• 2: MacOS ë˜ëŠ” Linuxë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, [brew, flox ë˜ëŠ” nix](./docs/install.md)ë¥¼ í†µí•´ llama.cppë¥¼ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë°©ë²• 3: Docker ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. [Docker ì„¤ëª…ì„œ](./docs/docker.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
- ë°©ë²• 4: [ë¦´ë¦¬ì¦ˆ](https://github.com/ggerganov/llama.cpp/releases)ì—ì„œ ë¯¸ë¦¬ ë¹Œë“œëœ ë°”ì´ë„ˆë¦¬ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ ì™„ì„±ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
llama-cli -m your_model.gguf -p "I believe the meaning of life is" -n 128

# Output:
# I believe the meaning of life is to find your own truth and to live in accordance with it. For me, this means being true to myself and following my passions, even if they don't align with societal expectations. I think that's what I love about yoga â€“ it's not just a physical practice, but a spiritual one too. It's about connecting with yourself, listening to your inner voice, and honoring your own unique journey.
```

ìì„¸í•œ ë§¤ê°œë³€ìˆ˜ ëª©ë¡ì€ [ì´ í˜ì´ì§€](./examples/main/README.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### ëŒ€í™” ëª¨ë“œ

ChatGPTì™€ ìœ ì‚¬í•œ ê²½í—˜ì„ ì›í•˜ì‹œë©´ `-cnv`ë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•˜ì—¬ ëŒ€í™” ëª¨ë“œë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
llama-cli -m your_model.gguf -p "You are a helpful assistant" -cnv

# Output:
# > hi, who are you?
# Hi there! I'm your helpful assistant! I'm an AI-powered chatbot designed to assist and provide information to users like you. I'm here to help answer your questions, provide guidance, and offer support on a wide range of topics. I'm a friendly and knowledgeable AI, and I'm always happy to help with anything you need. What's on your mind, and how can I assist you today?
#
# > what is 1+1?
# Easy peasy! The answer to 1+1 is... 2!
```

ê¸°ë³¸ì ìœ¼ë¡œ ì±— í…œí”Œë¦¿ì€ ì…ë ¥ ëª¨ë¸ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤. ë‹¤ë¥¸ ì±— í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ë ¤ë©´ `--chat-template NAME`ì„ ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•˜ì‹­ì‹œì˜¤. [ì§€ì›ë˜ëŠ” í…œí”Œë¦¿ ëª©ë¡](https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template)ì„ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

```bash
./llama-cli -m your_model.gguf -p "You are a helpful assistant" -cnv --chat-template chatml
```

ë˜í•œ, in-prefix, in-suffix ë° reverse-prompt ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ ìì‹ ì˜ í…œí”Œë¦¿ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
./llama-cli -m your_model.gguf -p "You are a helpful assistant" -cnv --in-prefix 'User: ' --reverse-prompt 'User:'
```

### ì›¹ ì„œë²„

[llama.cpp ì›¹ ì„œë²„](./examples/server/README.md)ëŠ” ê°€ë²¼ìš´ [OpenAI API](https://github.com/openai/openai-openapi) í˜¸í™˜ HTTP ì„œë²„ë¡œ, ë¡œì»¬ ëª¨ë¸ì„ ì œê³µí•˜ê³  ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ì— ì‰½ê²Œ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:

```bash
./llama-server -m your_model.gguf --port 8080

# Basic web UI can be accessed via browser: http://localhost:8080
# Chat completion endpoint: http://localhost:8080/v1/chat/completions
```

### ìƒí˜¸ ì‘ìš© ëª¨ë“œ

> [!ì°¸ê³ ]
> ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ì„ í˜¸í•˜ëŠ” ê²½ìš°, ëŒ€í™” ëª¨ë“œë¥¼ ìƒí˜¸ ì‘ìš© ëª¨ë“œ ëŒ€ì‹  ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•´ ë³´ì„¸ìš”

ì´ ëª¨ë“œì—ì„œëŠ” Ctrl+Cë¥¼ ëˆŒëŸ¬ ìƒì„±ì„ í•­ìƒ ì¤‘ë‹¨í•˜ê³  í•˜ë‚˜ ì´ìƒì˜ ì¤„ì˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì—¬ í˜„ì¬ ë§¥ë½ì— ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ `-r "ë°˜ì „ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´"` ë§¤ê°œë³€ìˆ˜ë¡œ *ë°˜ì „ í”„ë¡¬í”„íŠ¸*ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ìƒì„±ì—ì„œ ì •í™•í•œ í† í° ë¬¸ìì—´ì´ ë°˜ì „ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ê³¼ ì¼ì¹˜í•˜ëŠ” ê²½ìš° ì‚¬ìš©ì ì…ë ¥ì´ ìš”ì²­ë©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì‚¬ìš© ë°©ë²•ì€ LLaMAê°€ Aliceì™€ Bobê³¼ ê°™ì€ ì—¬ëŸ¬ ì‚¬ìš©ì ê°„ì˜ ëŒ€í™”ë¥¼ ì—ë®¬ë ˆì´íŠ¸í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ê³  `-r "Alice:"`ë¥¼ ì „ë‹¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ëª…ë ¹ì–´ë¡œ í˜¸ì¶œëœ ëª‡ ê°€ì§€ ìƒ· ìƒí˜¸ ì‘ìš©ì˜ ì˜ˆì…ë‹ˆë‹¤.

```bash
# default arguments using a 7B model
./examples/chat.sh

# advanced chat with a 13B model
./examples/chat-13B.sh

# custom arguments using a 13B model
./llama-cli -m ./models/13B/ggml-model-q4_0.gguf -n 256 --repeat_penalty 1.0 --color -i -r "User:" -f prompts/chat-with-bob.txt
```

ì‚¬ìš©ì ì…ë ¥ê³¼ ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ `--color` ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì£¼ì˜í•˜ì‹­ì‹œì˜¤. ë‹¤ë¥¸ ë§¤ê°œë³€ìˆ˜ëŠ” `llama-cli` ì˜ˆì œ í”„ë¡œê·¸ë¨ì˜ [README](examples/main/README.md)ì—ì„œ ìì„¸íˆ ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/1991296/224575029-2af3c7dc-5a65-4f64-a6bb-517a532aea38.png)

### ì§€ì†ì ì¸ ìƒí˜¸ ì‘ìš©

í”„ë¡¬í”„íŠ¸, ì‚¬ìš©ì ì…ë ¥ ë° ëª¨ë¸ ìƒì„±ì€ `./llama-cli` í˜¸ì¶œ ê°„ì— `--prompt-cache` ë° `--prompt-cache-all` ì„ ì‚¬ìš©í•˜ì—¬ ì €ì¥ ë° ì¬ê°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `./examples/chat-persistent.sh` ìŠ¤í¬ë¦½íŠ¸ëŠ” ì¥ì‹œê°„ ì§€ì†ë˜ê³  ì¬ê°œ ê°€ëŠ¥í•œ ì±„íŒ… ì„¸ì…˜ì„ ì§€ì›í•˜ì—¬ ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ì˜ˆì œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì´ˆê¸° ì±„íŒ… í”„ë¡¬í”„íŠ¸ë¥¼ ìºì‹œí•  íŒŒì¼ê³¼ ì±„íŒ… ì„¸ì…˜ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ë¥¼ ì œê³µí•´ì•¼ í•˜ë©°, `chat-13B.sh`ì™€ ë™ì¼í•œ ë³€ìˆ˜ë¥¼ ì„ íƒì ìœ¼ë¡œ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ìºì‹œëŠ” ìƒˆ ì±„íŒ… ì„¸ì…˜ì— ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ìºì‹œì™€ ì±„íŒ… ë””ë ‰í† ë¦¬ëŠ” ì´ˆê¸° í”„ë¡¬í”„íŠ¸(`PROMPT_TEMPLATE`) ë° ëª¨ë¸ íŒŒì¼ê³¼ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

```bash
# Start a new chat
PROMPT_CACHE_FILE=chat.prompt.bin CHAT_SAVE_DIR=./chat/default ./examples/chat-persistent.sh

# Resume that chat
PROMPT_CACHE_FILE=chat.prompt.bin CHAT_SAVE_DIR=./chat/default ./examples/chat-persistent.sh

# Start a different chat with the same prompt/model
PROMPT_CACHE_FILE=chat.prompt.bin CHAT_SAVE_DIR=./chat/another ./examples/chat-persistent.sh

# Different prompt cache for different prompt/model
PROMPT_TEMPLATE=./prompts/chat-with-bob.txt PROMPT_CACHE_FILE=bob.prompt.bin \
    CHAT_SAVE_DIR=./chat/bob ./examples/chat-persistent.sh
```

### ë¬¸ë²•ì„ ì‚¬ìš©í•œ ì œí•œëœ ì¶œë ¥

`llama.cpp`ëŠ” ëª¨ë¸ ì¶œë ¥ì„ ì œí•œí•˜ê¸° ìœ„í•œ ë¬¸ë²•ì„ ì§€ì›í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ëª¨ë¸ì´ JSONë§Œ ì¶œë ¥í•˜ë„ë¡ ê°•ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
./llama-cli -m ./models/13B/ggml-model-q4_0.gguf -n 256 --grammar-file grammars/json.gbnf -p 'Request: schedule a call at 8pm; Command:'
```

`grammars/` í´ë”ì—ëŠ” ëª‡ ê°€ì§€ ìƒ˜í”Œ ë¬¸ë²•ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ìì‹ ì˜ ë¬¸ë²•ì„ ì‘ì„±í•˜ë ¤ë©´ [GBNF ê°€ì´ë“œ](./grammars/README.md)ë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

ë” ë³µì¡í•œ JSON ë¬¸ë²•ì„ ì‘ì„±í•˜ë ¤ë©´ https://grammar.intrinsiclabs.ai/ì™€ ê°™ì€ ë¸Œë¼ìš°ì € ì•±ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ ì•±ì€ TypeScript ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‘ì„±í•˜ì—¬ GBNF ë¬¸ë²•ìœ¼ë¡œ ì»´íŒŒì¼í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ë¡œì»¬ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•±ì€ ì»¤ë®¤ë‹ˆí‹° íšŒì›ë“¤ì´ ê°œë°œí•˜ê³  ìœ ì§€ ê´€ë¦¬í•˜ê³  ìˆìœ¼ë¯€ë¡œ ë¬¸ì œë‚˜ ê¸°ëŠ¥ ì œì•ˆì€ [ì•± ë ˆí¬ì§€í† ë¦¬](http://github.com/intrinsiclabsai/gbnfgen)ì— ì œì¶œí•˜ì‹­ì‹œì˜¤. ì´ ë ˆí¬ì§€í† ë¦¬ì—ëŠ” ì œì¶œí•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

## ë¹Œë“œ

[llama.cpp ë¡œì»¬ ë¹Œë“œ ê°€ì´ë“œ](./docs/build.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## ì§€ì›ë˜ëŠ” ë°±ì—”ë“œ

| Backend | Target devices |
| --- | --- |
| [Metal](./docs/build.md#metal-build) | Apple Silicon |
| [BLAS](./docs/build.md#blas-build) | All |
| [BLIS](./docs/backend/BLIS.md) | All |
| [SYCL](./docs/backend/SYCL.md) | Intel and Nvidia GPU |
| [CUDA](./docs/build.md#cuda) | Nvidia GPU |
| [hipBLAS](./docs/build.md#hipblas) | AMD GPU |
| [Vulkan](./docs/build.md#vulkan) | GPU |

## ë„êµ¬

### ì¤€ë¹„ ë° ì–‘ìí™”

> [!ì°¸ê³ ]
> [GGUF-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo)ëŠ” ì„¤ì • ì—†ì´ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì–‘ìí™”í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Hugging Face ê³µê°„ì…ë‹ˆë‹¤. 6ì‹œê°„ë§ˆë‹¤ `llama.cpp` ë©”ì¸ì—ì„œ ë™ê¸°í™”ë©ë‹ˆë‹¤.

ê³µì‹ LLaMA 2 ê°€ì¤‘ì¹˜ë¥¼ ì–»ìœ¼ë ¤ë©´ <a href="#facebook-llama-2-ëª¨ë¸ì˜-íšë“ ë° ì‚¬ìš©">Facebook LLaMA 2 ëª¨ë¸ì˜ íšë“ ë° ì‚¬ìš©</a> ì„¹ì…˜ì„ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤. ë˜í•œ Hugging Faceì—ì„œ ë‹¤ì–‘í•œ ì‚¬ì „ ì–‘ìí™”ëœ `gguf` ëª¨ë¸ì´ ìˆìŠµë‹ˆë‹¤.

ì°¸ê³ : `convert.py`ëŠ” `examples/convert_legacy_llama.py`ë¡œ ì´ë™ë˜ì—ˆìœ¼ë©°, `Llama/Llama2/Mistral` ëª¨ë¸ ë° ê·¸ ìœ ë„ì²´ ì™¸ì—ëŠ” ë‹¤ë¥¸ ìš©ë„ë¡œ ì‚¬ìš©í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. LLaMA 3ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œí•œ LLaMA 3ì„ ì‚¬ìš©í•˜ë ¤ë©´ `convert_hf_to_gguf.py`ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.

ëª¨ë¸ì„ ì–‘ìí™”í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [ì´ ë¬¸ì„œ](./examples/quantize/README.md)ë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

### Perplexity (ëª¨ë¸ í’ˆì§ˆ ì¸¡ì •)

`perplexity` ì˜ˆì œë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ perplexityë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ë‚®ì€ perplexityê°€ ì¢‹ìŠµë‹ˆë‹¤).
ë” ìì„¸í•œ ë‚´ìš©ì€ [https://huggingface.co/docs/transformers/perplexity](https://huggingface.co/docs/transformers/perplexity)ë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

llama.cppë¥¼ ì‚¬ìš©í•˜ì—¬ perplexityë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹œë©´ [ì´ ë¬¸ì„œ](./examples/perplexity/README.md)ë¥¼ ì½ìœ¼ì‹­ì‹œì˜¤.

## ê¸°ì—¬

- ê¸°ì—¬ìëŠ” PRì„ ì—´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í˜‘ë ¥ìëŠ” `llama.cpp` ë ˆí¬ì§€í† ë¦¬ì˜ ë¸Œëœì¹˜ì— í‘¸ì‹œí•˜ê³  `master` ë¸Œëœì¹˜ë¡œ PRì„ ë³‘í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í˜‘ë ¥ìëŠ” ê¸°ì—¬ì— ë”°ë¼ ì´ˆëŒ€ë©ë‹ˆë‹¤.
- ë¬¸ì œ ë° PR ê´€ë¦¬ì— ëŒ€í•œ ë„ì›€ì€ ë§¤ìš° ê°ì‚¬í•©ë‹ˆë‹¤!
- ì²˜ìŒ ê¸°ì—¬í•˜ê¸°ì— ì í•©í•œ ì‘ì—…ì€ [good first issues](https://github.com/ggerganov/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
- ìì„¸í•œ ë‚´ìš©ì€ [CONTRIBUTING.md](CONTRIBUTING.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
- ì´ê²ƒì„ ë°˜ë“œì‹œ ì½ìœ¼ì„¸ìš”: [Inference at the edge](https://github.com/ggerganov/llama.cpp/discussions/205)
- ê´€ì‹¬ ìˆëŠ” ë¶„ë“¤ì„ ìœ„í•œ ê°„ëµí•œ ë°°ê²½ ì •ë³´: [Changelog podcast](https://changelog.com/podcast/532)

## ê¸°íƒ€ ë¬¸ì„œ

- [main (cli)](./examples/main/README.md)
- [ì„œë²„](./examples/server/README.md)
- [ì œíŒ¨ë””](./examples/jeopardy/README.md)
- [GBNF ë¬¸ë²•](./grammars/README.md)

**ê°œë°œ ë¬¸ì„œ**

- [ë¹Œë“œ ë°©ë²•](./docs/build.md)
- [Docker ì‚¬ìš©](./docs/docker.md)
- [Androidì—ì„œ ë¹Œë“œ](./docs/android.md)
- [ì„±ëŠ¥ ë¬¸ì œ í•´ê²°](./docs/development/token_generation_performance_tips.md)
- [GGML íŒ ë° íŠ¸ë¦­](https://github.com/ggerganov/llama.cpp/wiki/GGML-Tips-&-Tricks)

**ê¸°ë³¸ ë…¼ë¬¸ ë° ëª¨ë¸ ë°°ê²½ ì •ë³´**

ëª¨ë¸ ìƒì„± í’ˆì§ˆì— ë¬¸ì œê°€ ìˆëŠ” ê²½ìš°, LLaMA ëª¨ë¸ì˜ í•œê³„ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ë§í¬ ë° ë…¼ë¬¸ì„ í›‘ì–´ë³´ì‹­ì‹œì˜¤. íŠ¹íˆ ì ì ˆí•œ ëª¨ë¸ í¬ê¸°ë¥¼ ì„ íƒí•˜ê³  LLaMA ëª¨ë¸ê³¼ ChatGPT ì‚¬ì´ì˜ ì¤‘ìš”í•˜ê³  ë¯¸ë¬˜í•œ ì°¨ì´ì ì„ ì´í•´í•  ë•Œ ì¤‘ìš”í•©ë‹ˆë‹¤.
- LLaMA:
    - [Introducing LLaMA: A foundational, 65-billion-parameter large language model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
    - [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- GPT-3
    - [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- GPT-3.5 / InstructGPT / ChatGPT:
    - [Aligning language models to follow instructions](https://openai.com/research/instruction-following)
    - [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
