# 보안 정책

 - [**llama.cpp를 안전하게 사용하기**](#llamacpp를-안전하게-사용하기)
   - [신뢰되지 않는 모델](#신뢰되지-않는-모델)
   - [신뢰되지 않는 입력](#신뢰되지-않는-입력)
   - [데이터 프라이버시](#데이터-프라이버시)
   - [신뢰되지 않는 환경 또는 네트워크](#신뢰되지-않는-환경-또는-네트워크)
   - [다중 임대주 환경](#다중-임대주-환경)
 - [**취약점 신고**](#취약점-신고)

## llama.cpp를 안전하게 사용하는 방법

### 신뢰되지 않는 모델
신뢰되지 않는 모델을 실행할 때는 주의해야 합니다. 이 분류에는 알 수 없는 개발자가 만든 모델 또는 알 수 없는 출처에서 얻은 데이터를 사용하는 모델이 포함됩니다.

*신뢰되지 않는 모델은 항상 샌드박스(예: 컨테이너, 가상 머신)와 같은 안전하고 격리된 환경에서 실행해야 합니다.* 이렇게 하면 시스템이 잠재적으로 악의적인 코드로부터 보호됩니다.

> [!NOTE]
> 모델의 신뢰성은 이진이 아닙니다. 특정 모델과 사용 사례 및 위험 감수 수준에 따라 적절한 수준의 주의를 항상 결정해야 합니다.

### 신뢰되지 않는 입력

일부 모델은 다양한 입력 형식 (텍스트, 이미지, 오디오 등)을 수용합니다. 이러한 입력을 변환하는 라이브러리의 보안 수준은 다르므로, 스크립트 주입 위험을 완화하기 위해 모델을 격리하고 입력을 신중하게 사전 처리하는 것이 중요합니다.

신뢰되지 않는 입력을 처리할 때 최대 보안을 위해 다음과 같은 작업을 수행해야 할 수 있습니다.

* 샌드박싱: 인프런이 발생하는 환경을 격리합니다.
* 사전 분석: 모델이 프롬프트 주입에 노출되었을 때 기본적으로 어떻게 작동하는지 확인합니다. (예: [프롬프트 주입을 위한 fuzzing](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). 이를 통해 다음 주제에 대해 얼마나 노력해야 하는지에 대한 단서를 얻을 수 있습니다.
* 업데이트: LLaMA C++ 및 라이브러리를 최신 보안 패치로 업데이트합니다.
* 입력 정화: 데이터를 모델에 전달하기 전에 엄격하게 입력을 정화합니다. 이에는 다음과 같은 기술이 포함됩니다.
    * 검증: 허용되는 문자와 데이터 유형에 대한 엄격한 규칙을 적용합니다.
    * 필터링: 잠재적으로 악의적인 스크립트 또는 코드 조각을 제거합니다.
    * 인코딩: 특수 문자를 안전한 표현으로 변환합니다.
    * 검증: 잠재적인 스크립트 주입을 식별하는 도구를 실행합니다. (예: [프롬프트 주입 시도를 감지하는 모델](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)).

### 데이터 프라이버시

민감한 데이터가 누출되거나 무단으로 접근되는 것을 방지하기 위해 모델 실행을 샌드박스하는 것이 중요합니다. 즉, 모델을 안전하고 격리된 환경에서 실행하는 것을 의미하며, 이는 많은 공격 벡터를 완화하는 데 도움이 됩니다.

### 신뢰되지 않는 환경이나 네트워크

보안 및 격리된 환경에서 모델을 실행할 수 없거나 신뢰되지 않는 네트워크에 노출되어야 하는 경우 다음과 같은 보안 조치를 취하십시오.
* 다운로드된 모든 아티팩트(예: 사전 훈련된 모델 가중치)의 해시 값이 알려진 안전한 값과 일치하는지 확인하십시오.
* 네트워크를 통해 데이터를 전송하는 경우 데이터를 암호화하십시오.

### 다세대 환경

여러 모델을 공유 메모리로 병렬 실행할 계획이라면, 모델 간 상호 작용이나 서로의 데이터에 대한 액세스를 방지하는 것이 귀하의 책임입니다. 주요 우려 사항은 테넌트 분리, 리소스 할당, 모델 공유 및 하드웨어 공격입니다.

1. 테넌트 분리: 모델은 원치 않는 데이터 액세스를 방지하기 위해 강력한 분리 방법으로 별도로 실행되어야 합니다. 네트워크 분리는 격리에 필수적이며, 데이터 또는 모델에 대한 무단 액세스를 방지하고 악의적인 사용자가 다른 테넌트의 신원으로 실행하기 위해 그래프를 보내는 것을 방지합니다.

2. 리소스 할당: 하나의 모델에 의한 서비스 거부는 전체 시스템의 건강에 영향을 미칠 수 있습니다. 속도 제한, 액세스 제어 및 건강 모니터링과 같은 보호 조치를 구현하십시오.

3. 모델 공유: 다세대 모델 공유 설계에서는 테넌트 및 사용자가 다른 사람이 제공한 코드를 실행하는 데 따른 보안 위험을 이해해야 합니다. 악의적인 모델을 감지하는 신뢰할 수 있는 방법이 없기 때문에 모델 실행을 샌드박스하는 것이 위험을 완화하는 권장 사항입니다.

4. 하드웨어 공격: GPU 또는 TPU도 공격받을 수 있습니다. [연구](https://scholar.google.com/scholar?q=gpu+side+channel)에 따르면 GPU에 대한 부수적인 채널 공격이 가능하며, 동시에 동일한 시스템에서 실행 중인 다른 모델이나 프로세스에서 데이터가 유출될 수 있습니다.

## 취약점 보고

[llama.cpp를 안전하게 사용하기](#using-llamacpp-securely) 아래 주제들은 LLaMA C++의 취약점으로 간주되지 않습니다.

<!-- normal version -->
그러나 이 프로젝트에서 보안 취약점을 발견한 경우, 개인적으로 보고해주세요. **공개적으로 문제를 공개하지 마세요.** 이는 공개되기 전에 문제를 해결하기 위해 함께 작업할 시간을 주며, 패치가 출시되기 전에 악용될 가능성을 줄입니다.

[보안 자문](https://github.com/ggerganov/llama.cpp/security/advisories/new)으로 개인적으로 보고해주세요.

이 프로젝트는 합리적인 노력을 기반으로 하는 자원봉사자 팀이 유지 관리합니다. 따라서 공개적으로 노출되기 전에 최소 90일 동안 해결책을 찾도록 해주세요.
