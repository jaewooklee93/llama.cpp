
# SimpleChat

모두를 위한 인간에 의한 개발.

## 빠른 시작

빌드 디렉토리에서 실행하려면

bin/llama-server -m path/model.gguf --path ../examples/server/public_simplechat

자세한 내용은 아래를 참조하십시오.

## 개요

이 간단한 웹 프론트엔드는 서버의 `/completions` 또는 `/chat/completions` 엔드포인트를 간단하게 트리거하고 테스트할 수 있도록 합니다.
최소한의 코드로 일반적인 코드베이스에서 작동합니다. 또한, 기본적인 수준에서 AI LLM 모델을 사용하여 단일 또는 여러 개의 독립적인 왕복 채팅을 허용하려고 합니다.

이를 통해 AI 모델의 응답인 생성된 텍스트를 한 번에 볼 수 있습니다. 생성이 완료된 후 또는 서버/AI 모델에서 스트리밍 방식으로 생성되는 동안 볼 수 있습니다.

![채팅 및 설정 화면](./simplechat_screens.webp "채팅 및 설정 화면")

채팅이 진행되는 동안 로컬로 채팅 세션을 자동으로 저장하고, 나중에 SimpleChat를 열면 일치하는 세션이 있으면 해당 세션을 복원할 수 있는 옵션이 제공됩니다.

UI는 사용 가능한 디스플레이 공간에 맞춰 레이아웃을 조정할 수 있는 반응형 웹 디자인을 따릅니다.

개발자/사용자가 브라우저의 개발 도구 콘솔에서 gMe 멤버를 업데이트하여 일부 동작을 제어할 수 있습니다. 동시에 사용자에게 직접 유용한 설정은 제공된 설정 UI를 통해 변경할 수도 있습니다.

참고: 현재 웹 서비스 API는 모델 컨텍스트 길이를 직접 노출하지 않으므로 클라이언트 로직은 오래된 메시지를 적응적으로 제거하거나 내용 요약으로 대체하는 기능을 제공하지 않습니다. 그러나 간단한 오래된 메시지 제거 로직이 있는 선택 사항이 있습니다.

참고: 요청에 전달되는 옵션은 현재 주로 온도, 최대 토큰 및 스트림 옵션을 설정합니다. 그러나 필요에 따라 사용자는 js 파일 또는 gMe의 해당 멤버를 업데이트할 수 있습니다.

참고: 이를 사용하여 오픈AI API 웹 서비스 `/chat/completions` 엔드포인트와 매우 제한된 방식으로 채팅할 수 있습니다. 설정 UI에서 모델, 오픈AI URL 및 인증 토큰을 설정해야 합니다.


## 사용법

이 웹 프론트엔드를 서버 자체로 직접 실행하거나, HTTP(S)를 통해 서버를 구성하는 데 내장된 웹 프론트엔드를 추가하려는 경우, Python의 http 모듈과 같은 것을 사용하여 이 웹 프론트엔드를 실행할 수 있습니다.

### examples/server를 사용하여 실행하기

./llama-server -m path/model.gguf --path examples/server/public_simplechat [--port PORT]

### Python3의 서버 모듈을 사용하여 실행하기

먼저 examples/server에서 실행합니다.
* ./llama-server -m path/model.gguf

다음으로 examples/server/public_simplechat에서 웹 프론트엔드를 실행합니다.
* cd ../examples/server/public_simplechat
* python3 -m http.server PORT

### 프론트엔드 사용 방법

 로컬 브라우저에서 이 간단한 웹 프론트엔드를 열어보세요.

* http://127.0.0.1:PORT/index.html

 내부에서

* 원하는 경우, 여러 개의 기본 글로벌 설정을 변경할 수 있습니다.
  * 기본 URL (예: IP 주소/도메인 이름, 포트)
  * 채팅 (기본) vs 완성 모드
  * 응답에서 불필요한 텍스트를 제거할지 여부
  * 서버/AI 모델에 전달되는 챗 히스토리의 양
  * 단일 요청 또는 스트리밍 모드

* 완성 모드의 경우
  * 일반적으로 완성 모드에서는 시스템 프롬프트를 사용하지 않습니다.
  * 논리적으로 기본적으로 각 역할의 메시지에 대한 특정 역할을 나타내는 "ROLE: " 접두어를 삽입하지 않습니다. 모델이 사용자 역할 메시지에 대한 어떤 접두어를 필요로 하는 경우, 사용자는 메시지를 입력할 때 필요한 접두어를 명시적으로 추가해야 합니다. 마찬가지로 모델이 AI 모델 응답을 트리거하기 위해 어떤 접두어를 필요로 하는 경우, 사용자는 동일한 접두어를 입력해야 합니다. 이는 논리를 간단하게 유지하면서도 사용자가 모델에 보내는 메시지에 대한 임시 템플릿/태그 요구 사항을 관리할 수 있는 유연성을 제공합니다.
  * 논리는 프롬프트 메시지에 줄 바꿈을 삽입하지 않습니다. 그러나 사용자 메시지가 여러 역할의 메시지를 포함하는 경우, 역할의 메시지 간에 줄 바꿈을 삽입하여 명확하게 구분됩니다.
  * /completions 엔드포인트가 일반적으로 추가적인 챗 템플릿을 추가하지 않기 때문에, 위의 설정은 사용자가 모델과의 손뼉 테스트를 위해 임의의 태그/특수 토큰이 포함된 단일/다중 메시지 조합을 만들 수 있도록 합니다. 또는 사용자는 일반적인 완성 관련 쿼리에 대해서도 사용할 수 있습니다.

* 시스템 프롬프트를 제공하려면, 사용자 쿼리를 입력하기 전에 먼저 입력하십시오. 일반적으로 완성 모드는 시스템 프롬프트가 필요하지 않지만, 채팅 모드는 적절한 시스템 프롬프트를 사용하면 더 나은/흥미로운 응답을 생성할 수 있습니다.
  * 만약 chat.add_system_begin 을 사용한다면
    * 사용자 쿼리와 함께 시스템 프롬프트가 제출된 후에는 시스템 프롬프트를 변경할 수 없습니다.
    * 사용자 쿼리를 제출한 후에는 시스템 프롬프트를 설정할 수 없습니다.
  * 만약 chat.add_system_anytime 을 사용한다면
    * 사용자는 채팅 중 언제든지 시스템 프롬프트를 변경할 수 있습니다. 시스템 프롬프트의 내용을 변경하면 됩니다.
    * 그 결과 업데이트된/변경된 시스템 프롬프트가 채팅 세션에 삽입됩니다.
    * 이를 통해 이후 사용자의 채팅이 위에서 설정된 새로운 시스템 프롬프트에 의해 제어됩니다.

* 쿼리를 입력하고 Enter 키를 누르거나 제출 버튼을 클릭하십시오.
  AI 모델에 엔터(\n)를 포함시키려면 Shift+Enter를 사용하십시오.

* 논리가 서버와 통신하고 응답을 가져오는 동안 기다립니다.
  * 이 기간 동안 사용자는 새 쿼리를 입력할 수 없습니다.
  * 사용자 입력 상자는 비활성화되고 작업 중인 메시지가 표시됩니다.
  * 불필요한 텍스트를 제거하는 기능이 활성화된 경우, 논리는 반복되는 텍스트와 같은 불필요한 텍스트를 일정 정도 제거하려고 합니다.

* 채팅 히스토리와 시스템 프롬프트를 초기화하고 새로 시작하려면 페이지를 새로 고칩니다.

* NewChat을 사용하면 독립적인 채팅 세션을 시작할 수 있습니다.
  * 기본적으로 두 개의 독립적인 채팅 세션이 설정됩니다.

* 인쇄하려면 ChatHistoryInCtxt를 Full로 설정하고 원하는 채팅 세션 버튼을 클릭하면 해당 세션의 전체 채팅 히스토리가 표시됩니다.


## 개발 노트

### 이것의 이유

기본적인 용도로 사용하기 쉽도록 하면서도, 웹 프론트엔드 배경이 없는 개발자(따라서 템플릿/엔드-유저-구체적인-언어 확장 기반 흐름에 익숙하지 않을 수 있는)도 쉽게 이해하고 사용할 수 있도록 간단하게 만들었습니다. 이를 통해 개발자가 탐색하고 실험할 수 있도록 합니다.

또한 개발자들이 탐색하고 실험하는 데 도움을 주기 위해, 개발 도구/콘솔 또는 제공된 최소 설정 UI(몇 가지 측면에 대해)를 사용하여 동작을 쉽게 변경할 수 있는 유연성을 제공합니다. 몇 가지 엔드포인트와 그 주변의 아이디어/의미에 대한 탐색을 위한 기본적인 논리 구조가 구현되었습니다.


### 일반 정보

Me/gMe는 동작을 제어하는 설정을 하나의 객체로 통합합니다.
브라우저 개발 도구/콘솔을 사용하여 현재 설정을 확인하고 변경하거나 업데이트할 수 있습니다.
문서 객체에 연결되어 있습니다. 이 중 일부는 설정 UI를 사용하여 업데이트할 수도 있습니다.

  baseURL - 요청을 보낼 도메인 이름/IP 주소 및 포트

  bStream - oneshot-at-end와 live-stream-as-its-generated을 통제하여 생성된 응답을 수집하고 표시합니다.

    이 로직은 서버에서 전송되는 텍스트가 UTF-8 인코딩을 따르는 것으로 가정합니다.

    스트리밍 모드에서 예외가 발생하면 로직이 예외를 잡고 생성된 텍스트가 손실되지 않도록 합니다.

      매우 긴 텍스트가 생성되면 사용자가 일정 시간 동안 상호 작용하지 않고 기계가 절전 모드로 들어가거나, 플랫폼이 네트워크 연결을 중단하여 예외가 발생할 수 있습니다.

  apiEP - 서버/AI 모델에서 제공하는 /completions 또는 /chat/completions 엔드포인트 중 선택합니다.

  bCompletionFreshChatAlways - Completion 모드가 서버와 통신할 때 완전한/슬라이딩 창 기록을 수집하거나 최신 사용자 쿼리/메시지만 보내는지 여부를 결정합니다.

  bCompletionInsertStandardRolePrefix - Completion 모드가 /Completion 엔드포인트에 대한 프롬프트 필드에 삽입되는 메시지에 대한 역할 관련 접두어를 삽입하는지 여부를 결정합니다.

  bTrimGarbage - 생성된 AI 응답의 끝에 있는 불필요한 반복을 잘라내거나 그대로 유지하는지 여부를 결정합니다. 활성화되면 잘라내어 이후 채팅 기록의 일부로 다시 전송되지 않습니다. 동시에 실제 잘라낸 텍스트가 사용자에게 한 번 표시되므로 응답에 유용한 정보/데이터가 있는지 확인할 수 있습니다.

    사용자는 AI 모델에 마지막 응답을 계속하도록 요청할 수 있으며 (채팅 기록이 채팅 기록 맥락 설정의 일부로 활성화된 경우), AI 모델은 잘라낸 부분부터 계속해서 시작할 가능성이 높습니다. 이를 통해 긴 응답이 간접적으로 복구되거나 계속될 수 있습니다.

    히스토그램/빈도 기반 잘라내기 로직은 현재 영어 언어에 대해 조정되어 있습니다.

    is-it-a-alpabetic|numeral-char regex match logic.

  apiRequestOptions - /chat/completions 또는 /completions 엔드포인트에 관계없이 api 요청과 함께 전송할 옵션/필드 목록을 유지합니다.

    서버/AI 모델에 추가 옵션/필드를 전송하거나 기존 옵션 값을 수정하거나 제거하려면 현재 이 글로벌 변수를 브라우저의 개발 도구/콘솔에서 업데이트할 수 있습니다.

    apiRequestOptions의 문자열, 숫자 및 불린 필드는 사용자가 실행 시간에 직접 gMe.apiRequestOptions를 수정하여 추가한 필드를 포함하여 자동으로 UI 항목이 생성됩니다.

    예시/서버에서 지원하는 cache_prompt 옵션은 사용자가 제어할 수 있으므로 시스템 프롬프트 및 채팅 기록에 대한 캐싱이 지원되는 경우 사용할 수 있습니다. 채팅 기록 슬라이딩 창이 활성화되면 캐시_프롬프트 논리는 모델, 위치 인코딩, 주의 메커니즘 등과 관련된 측면에 따라 백엔드에서 동일하게 작동하지 않을 수 있습니다. 그러나 시스템 프롬프트는 캐싱의 이점을 받아야 합니다.

  headers - 요청이 서버로 전송될 때 전송되는 HTTP 헤더 목록을 유지합니다. 기본적으로 Content-Type은 application/json으로 설정됩니다. 또한 Authorization 항목이 제공되며, 설정 UI를 사용하여 필요에 따라 설정할 수 있습니다.

  iRecentUserMsgCnt - AI 모델 측에서 컨텍스트 창 로드를 제한하기 위한 간단한 슬라이딩 창입니다. 기본적으로 비활성화되어 있습니다. 그러나 활성화되면 최신 시스템 메시지 외에도 iRecentUserMsgCnt 개의 최신 사용자 메시지만이 최신 시스템 프롬프트 및 AI 모델의 응답 이후에 AI 모델에 전송됩니다. 즉, 활성화되면 최신 시스템 메시지/프롬프트 이후의 사용자 메시지만 고려됩니다.

    이 지정된 슬라이딩 창 사용자 메시지 카운트에는 최신 사용자 쿼리도 포함됩니다.
    <0 : 서버에 전체 채팅 기록을 전송합니다.
     0 : 서버에 시스템 메시지만 전송합니다.
    >0 : 최신 시스템 프롬프트부터 제한된 카운트만큼 최신 채팅 기록을 전송합니다.


 gMe의 iRecentUserMsgCnt 및 apiRequestOptions.max_tokens/n_predict를 사용하면 AI 모델의 컨텍스트 창 로드에 대한 영향을 간단하고 촌스럽게 제어할 수 있습니다. 서버에서 AI 모델을 로드할 때 사용되는 컨텍스트 크기를 제어하는 것도 고려해야 합니다.


 때로는 브라우저가 파일 캐싱을 고집하여 HTML/CSS/JS 업데이트가 표시되지 않을 수 있습니다. 또한 브라우저에서 페이지를 새로 고치거나 사이트 데이터를 삭제하는 것만으로도 모든 경우에 사이트 캐싱을 직접 무효화하지는 않습니다. 최악의 경우에는 포트를 변경해야 할 수도 있습니다. 또는 브라우저의 개발 도구에서 캐싱을 완전히 비활성화할 수도 있습니다.


 현재 서버와의 통신은 특정 채팅 세션의 일부가 아니라 글로벌로 유지됩니다. 따라서 설정에서 서버 IP/URL을 변경하면 모든 채팅 세션이 새 서버로 자동으로 전환됩니다. 


 chat.add_system_begin/anytime 간을 전환하면 대화 중 언제든 시스템 프롬프트를 변경할 수 있는지, 또는 시작 시에만 변경할 수 있는지 제어할 수 있습니다.


### 기본 설정

기본적으로 사용자 경험을 조금 더 향상시키기 위해 설정이 되어 있습니다. 그러나 AI 모델의 서버를 테스트하는 개발자는 이 값을 변경하고 싶을 수 있습니다.

`iRecentUserMsgCnt`를 사용하면 서버/AI 모델에 전송되는 채팅 기록 맥락을 줄여 시스템 프롬프트, 이전 사용자 요청 및 AI 응답, 현재 사용자 요청만으로 합니다. 이렇게 하면 응답에 쓸모없거나 반복되는 내용이 있더라도 다음 질문/요청/쿼리 이후의 내용에 영향을 미치지 않습니다. `trim garbage` 옵션 또한 맥락에 있는 쓸모없는 내용으로 인한 문제를 어느 정도 방지하려고 합니다.

`max_tokens`를 1024로 설정하여 상대적으로 큰 이전 응답이 다음 쿼리-응답에 사용 가능한 공간을 차지하지 않도록 합니다. 그러나 서버가 시작될 때 모델 맥락 크기를 1k 이상으로 설정해야 안전합니다.

  `examples/server/completions` 엔드포인트는 `max_tokens`를 받지 않고 현재는 내부 `n_predict`를 사용합니다. 나중에 서버 측 `completions` 엔드포인트 처리 코드에 `max_tokens`를 추가할 수도 있습니다.

참고: `apiRequestOptions`의 `frequency/presence penalty` 필드를 사용자 쿼리와 함께 서버에 전송되는 필드 세트에 대해 실험하여 모델이 생성된 텍스트 응답에서 반복에 대해 어떻게 행동하는지 확인할 수 있습니다.


엔드 사용자는 브라우저의 개발 도구/콘솔에서 `gMe`를 편집하거나 제공된 설정 UI(UI를 통해 노출되는 설정)를 사용하여 이러한 동작을 변경할 수 있습니다.


### OpenAi / 동등한 API 웹 서비스

OpenAI/동등한 API 웹 서비스의 /chat/completions 엔드포인트와 손을 잡을 수 있을 수 있습니다.
아래 설정을 통해 최소한의 채팅 실험을 수행할 수 있습니다.

* 설정 UI에서 baseUrl
  * https://api.openai.com/v1 또는 유사한 값

* 요청 바디 - gMe.apiRequestOptions
  * 모델 (설정 UI)
  * 필요에 따라 추가 필드

* 요청 헤더 - gMe.headers
  * Authorization (설정 UI에서 사용 가능)
    * Bearer THE_OPENAI_API_KEY
  * 추가적인 옵션 헤더 항목 (예: "OpenAI-Organization", "OpenAI-Project")

참고: 무료 계층 API 테스트가 제공되지 않아 테스트되지 않았습니다. 그러나 논리적으로 이것이 작동할 수 있습니다.


## 끝맺음

모두에게 이익을 가져다주려는 노력을 하는 모든 오픈소스 및 오픈 모델 개발자들에게 감사 인사를 전합니다.
