# Perplexity

`perplexity` 예제는 주어진 텍스트 코퍼스에 대한 언어 모델의 복잡도 값을 계산하는 데 사용할 수 있습니다.
복잡도는 모델이 다음 토큰을 예측하는 능력을 나타내며, 값이 낮을수록 좋습니다.
참고로 복잡도는 모델이 다른 토큰화기를 사용하는 경우 **직접 비교할 수 없습니다.**
또한, fine-tuning은 일반적으로 출력의 인간 평가된 품질이 향상되더라도 복잡도 값이 더 높아질 수 있습니다.

llama.cpp 내에서 기본 모델의 복잡도는 예를 들어 양자화된 모델과 FP16 모델의 품질 손실을 판단하는 데 주로 사용됩니다.
기여자들 사이의 관습은 `scripts/get-wikitext-2.sh`로 얻을 수 있는 Wikitext-2 테스트 세트를 사용하여 테스트하는 것입니다.
숫자가 나열된 경우 모든 명령줄 인수와 컴파일 옵션이 기본값으로 유지됩니다.
llama.cpp 숫자는 정확한 값이 구현 세부 사항에 크게 의존하기 때문에 다른 프로젝트의 숫자와 **직접 비교할 수 없습니다.**

기본적으로는 평균 복잡도 값과 해당 불확실성만 계산됩니다.
불확실성은 각 토큰에 대한 "올바른" 로짓에 대한 가우시안 분포를 가정하고 오류 전파를 적용하여 실험적으로 결정됩니다.

더 많은 통계를 얻으려면 FP16 버전의 모델에서 로짓을 기록할 수 있습니다.
이렇게 하려면 `perplexity`에 `--kl-divergence-base path/to/logit/binary/file.kld`를 제공합니다.
프로그램은 제공된 경로에 로짓을 이진 형식으로 저장합니다.
**로짓 파일은 매우 크며, Wikitext-2 테스트 세트를 사용하면 LLaMA 2의 경우 11 GiB, LLaMA 3의 경우 37 GiB가 됩니다.**
로짓 파일을 얻으면 양자화된 모델, `--kl-divergence-base`를 통해 로짓 파일을 제공하고, 마지막으로 `--kl-divergence` 인수를 제공하여 프로그램이 FP16과 양자화된 로짓 분포의 Kullback-Leibler 발산을 계산하도록 합니다.
이것은 FP16과 양자화된 로짓 분포가 얼마나 유사한지를 나타내는 지표로, 0은 분포가 동일하다는 것을 의미합니다.
평균 KL 발산의 불확실성은 KL 발산이 토큰별로 가우시안 분포를 따른다고 가정하여 계산됩니다.

`--kl-divergence`를 사용하면 다음과 같은 통계도 계산됩니다.

* 평균 FP16 PPL과 양자화된 PPL의 비율. 불확실성은 로짓에서 추정되고, 그런 다음 전파됩니다. 이 지표의 로그도 계산되고 출력됩니다. 로짓 분포가 동일하면 0입니다.
* 평균 FP16 PPL과 양자화된 PPL의 차이. 불확실성은 로짓에서 추정되고, 그런 다음 전파됩니다.
* "올바른" 토큰 확률의 평균 변화. 긍정적인 값은 모델이 예측에 더 능숙해진다는 것을 의미하며, 부정적인 값은 모델이 예측에 더 부족해진다는 것을 의미합니다.
* 두 모델 간의 "올바른" 토큰 확률의 피어슨 상관계수.
* "올바른" 토큰 확률의 변화의 백분위수. 긍정적인 값은 모델이 예측에 더 능숙해진다는 것을 의미하며, 부정적인 값은 모델이 예측에 더 부족해진다는 것을 의미합니다. 양자화로 인한 소음과 품질 손실을 판단하는 데 사용할 수 있습니다. 백분위수가 대칭이라면 양자화는 단순히 소음을 추가하는 것과 같습니다. 부정적인 값이 긍정적인 값보다 크게 떨어진다면 모델이 양자화로 인해 실제로 더 나빠지고 있음을 나타냅니다.
* 토큰 확률의 변화의 제곱근 평균. 양자화가 토큰 확률에 가우시안 소음을 일으키는 것으로 가정하면 이 값은 해당 소음의 표준 편차입니다. 값의 불확실성은 토큰 확률의 변화가 가우시안 분포를 따른다고 계산됩니다. 관련 토론: https://github.com/ggerganov/llama.cpp/discussions/2875 .
* 동일한 top p: 두 모델 모두에서 토큰이 가장 높은 확률을 할당받은 빈도의 백분율입니다. 불확실성은 이항 분포의 가우시안 근사에서 계산됩니다.

## LLaMA 3 8b 점수표

| Revision | f364eb6f           |
|:---------|:-------------------|
| Backend  | CUDA               |
| CPU      | AMD Epyc 7742      |
| GPU      | 1x NVIDIA RTX 4090 |

CUDA 백엔드를 사용하여 생성된 결과이며, FP16에 대한 Kullback-Leibler 분산에 따라 정렬되었습니다.
"WT" 중요 행렬은 다양한 수의 Wikitext 토큰을 사용하여 생성되었으며, [여기](https://huggingface.co/JohannesGaessler/llama.cpp_importance_matrices/blob/main/imatrix-llama_3-8b-f16-2.7m_tokens.dat)에서 찾을 수 있습니다.
참고: perplexity 외 다른 모든 지표를 계산하는 데 사용되는 FP16 logits는 실행 간에 이진 파일로 저장됩니다.
공간을 절약하기 위해 이 파일에는 정확한 FP32 logits가 포함되어 있지 않으며, 대신 16비트 무리수 정수로 변환됩니다 (일부 스케일링이 적용됩니다).
따라서 "f16" 결과는 이 변환으로 인해 발생하는 차이만을 나타냅니다.

| Quantization | imatrix | Model size [GiB] | PPL                    | ΔPPL                   | KLD                   | Mean Δp           | RMS Δp           |
|--------------|---------|------------------|------------------------|------------------------|-----------------------|-------------------|------------------|
| f16          | None    |            14.97 | 6.233160 ±   0.037828  | 0.001524 ±   0.000755  | 0.000551 ±   0.000002 |  0.001 ± 0.002 %  | 0.787 ± 0.004 %  |
| q8_0         | None    |             7.96 | 6.234284 ±   0.037878  | 0.002650 ±   0.001006  | 0.001355 ±   0.000006 | -0.019 ± 0.003 %  | 1.198 ± 0.007 %  |
| q6_K         | None    |             6.14 | 6.253382 ±   0.038078  | 0.021748 ±   0.001852  | 0.005452 ±   0.000035 | -0.007 ± 0.006 %  | 2.295 ± 0.019 %  |
| q5_K_M       | None    |             5.33 | 6.288607 ±   0.038338  | 0.056974 ±   0.002598  | 0.010762 ±   0.000079 | -0.114 ± 0.008 %  | 3.160 ± 0.031 %  |
| q5_K_S       | None    |             5.21 | 6.336598 ±   0.038755  | 0.104964 ±   0.003331  | 0.016595 ±   0.000122 | -0.223 ± 0.010 %  | 3.918 ± 0.036 %  |
| q5_1         | None    |             5.65 | 6.337857 ±   0.038677  | 0.106223 ±   0.003476  | 0.018045 ±   0.000139 | -0.287 ± 0.011 %  | 4.123 ± 0.039 %  |
| q5_0         | None    |             5.21 | 6.363224 ±   0.038861  | 0.131591 ±   0.003894  | 0.022239 ±   0.000166 | -0.416 ± 0.012 %  | 4.634 ± 0.043 %  |
| q4_K_M       | WT 10m  |             4.58 | 6.382937 ±   0.039055  | 0.151303 ±   0.004429  | 0.028152 ±   0.000240 | -0.389 ± 0.014 %  | 5.251 ± 0.049 %  |
| q4_K_M       | None    |             4.58 | 6.407115 ±   0.039119  | 0.175482 ±   0.004620  | 0.031273 ±   0.000238 | -0.596 ± 0.014 %  | 5.519 ± 0.050 %  |
| q4_K_S       | WT 10m  |             4.37 | 6.409697 ±   0.039189  | 0.178064 ±   0.004744  | 0.031951 ±   0.000259 | -0.531 ± 0.015 %  | 5.645 ± 0.051 %  |
| iq4_NL       | WT 10m  |             4.35 | 6.455593 ±   0.039630  | 0.223959 ±   0.005201  | 0.035742 ±   0.000288 | -0.590 ± 0.016 %  | 5.998 ± 0.054 %  |
| iq4_XS       | WT 10m  |             4.14 | 6.459705 ±   0.039595  | 0.228071 ±   0.005207  | 0.036334 ±   0.000284 | -0.668 ± 0.016 %  | 6.044 ± 0.054 %  |
| q4_K_S       | None    |             4.37 | 6.500529 ±   0.039778  | 0.268895 ±   0.005638  | 0.043136 ±   0.000314 | -0.927 ± 0.017 %  | 6.562 ± 0.055 %  |
| q4_1         | None    |             4.78 | 6.682737 ±   0.041285  | 0.451103 ±   0.008030  | 0.071683 ±   0.000505 | -0.927 ± 0.017 %  | 8.512 ± 0.063 %  |
| q4_0         | None    |             4.34 | 6.700147 ±   0.041226  | 0.468514 ±   0.007951  | 0.071940 ±   0.000491 | -1.588 ± 0.022 %  | 8.434 ± 0.061 %  |
| q3_K_L       | WT 10m  |             4.03 | 6.671223 ±   0.041427  | 0.439590 ±   0.008154  | 0.073077 ±   0.000529 | -0.940 ± 0.023 %  | 8.662 ± 0.064 %  |
| q3_K_M       | WT 10m  |             3.74 | 6.734255 ±   0.041838  | 0.502622 ±   0.008901  | 0.084358 ±   0.000588 | -1.198 ± 0.024 %  | 9.292 ± 0.065 %  |
| q3_K_L       | None    |             4.03 | 6.787876 ±   0.042104  | 0.556242 ±   0.009171  | 0.087176 ±   0.000614 | -1.532 ± 0.025 %  | 9.432 ± 0.067 %  |
| q3_K_M       | None    |             3.74 | 6.888498 ±   0.042669  | 0.656864 ±   0.010071  | 0.101913 ±   0.000677 | -1.990 ± 0.026 %  | 10.203 ± 0.068 % |
| iq3_M        | WT 10m  |             3.53 | 6.898327 ±   0.041643  | 0.666694 ±   0.009449  | 0.102534 ±   0.000663 | -3.178 ± 0.026 %  | 10.513 ± 0.066 % |
| iq3_S        | WT 10m  |             3.42 | 6.965501 ±   0.042406  | 0.733867 ±   0.010245  | 0.111278 ±   0.000710 | -3.066 ± 0.027 %  | 10.845 ± 0.068 % |
| iq3_XS       | WT 10m  |             3.28 | 7.163043 ±   0.043772  | 0.931409 ±   0.012084  | 0.138693 ±   0.000857 | -3.667 ± 0.031 %  | 12.148 ± 0.070 % |
| iq3_XXS      | WT 10m  |             3.05 | 7.458436 ±   0.046404  | 1.226803 ±   0.015234  | 0.183625 ±   0.001042 | -3.918 ± 0.035 %  | 13.836 ± 0.074 % |
| q3_K_S       | WT 10m  |             3.41 | 7.602878 ±   0.046848  | 1.371244 ±   0.015688  | 0.199821 ±   0.001008 | -5.046 ± 0.037 %  | 14.980 ± 0.070 % |
| q3_K_S       | None    |             3.41 | 7.863786 ±   0.048885  | 1.632152 ±   0.017733  | 0.228217 ±   0.001079 | -5.604 ± 0.038 %  | 15.541 ± 0.070 % |
| iq2_M        | WT 10m  |             2.74 | 8.600799 ±   0.055124  | 2.369166 ±   0.025244  | 0.325989 ±   0.00160  | -6.463 ± 0.046 %  | 18.519 ± 0.080 % |
| q2_K         | WT 10k  |             2.96 | 8.652290 ±   0.055572  | 2.420657 ±   0.025587  | 0.331393 ±   0.001562 | -6.606 ± 0.046 %  | 18.790 ± 0.078 % |
| q2_K         | WT 100k |             2.96 | 8.641993 ±   0.055406  | 2.410359 ±   0.025495  | 0.331672 ±   0.001569 | -6.628 ± 0.047 %  | 18.856 ± 0.078 % |
| q2_K         | WT 10m  |             2.96 | 8.647825 ±   0.055610  | 2.416191 ±   0.025683  | 0.332223 ±   0.001572 | -6.500 ± 0.047 %  | 18.881 ± 0.078 % |
| q2_K         | WT 1m   |             2.96 | 8.674365 ±   0.055743  | 2.442732 ±   0.025843  | 0.335308 ±   0.001576 | -6.634 ± 0.047 %  | 19.009 ± 0.079 % |
| q2_K         | WT 1k   |             2.96 | 8.682605 ±   0.055916  | 2.450972 ±   0.026069  | 0.337093 ±   0.001596 | -6.596 ± 0.047 %  | 18.977 ± 0.079 % |
| q2_K_S       | WT 10m  |             2.96 | 9.323778 ±   0.061551  | 3.092145 ±   0.031914  | 0.403360 ±   0.001787 | -7.131 ± 0.049 %  | 20.050 ± 0.081 % |
| q2_K_S       | WT 1m   |             2.96 | 9.329321 ±   0.061378  | 3.097688 ±   0.031816  | 0.403590 ±   0.001797 | -7.289 ± 0.049 %  | 20.123 ± 0.081 % |
| q2_K_S       | WT 100k |             2.96 | 9.362973 ±   0.061740  | 3.131339 ±   0.032169  | 0.408367 ±   0.001802 | -7.198 ± 0.050 %  | 20.132 ± 0.081 % |
| q2_K_S       | WT 10k  |             2.96 | 9.376479 ±   0.062045  | 3.144846 ±   0.032464  | 0.408662 ±   0.001819 | -7.141 ± 0.050 %  | 20.120 ± 0.081 % |
| q2_K_S       | WT 1k   |             2.96 | 9.415200 ±   0.062475  | 3.183567 ±   0.032993  | 0.415865 ±   0.001846 | -7.153 ± 0.050 %  | 20.311 ± 0.082 % |
| iq2_S        | WT 10m  |             2.56 | 9.650781 ±   0.063209  | 3.419148 ±   0.034017  | 0.439197 ±   0.001976 | -8.319 ± 0.052 %  | 21.491 ± 0.083 % |
| q2_K         | None    |             2.96 | 9.751568 ±   0.063312  | 3.519934 ±   0.033863  | 0.445132 ±   0.001835 | -9.123 ± 0.051 %  | 21.421 ± 0.079 % |
| iq2_XS       | WT 10m  |             2.43 | 10.761424 ±   0.071056 | 4.529791 ±   0.042229  | 0.546290 ±   0.002133 | -10.576 ± 0.056 % | 23.872 ± 0.082 % |
| iq2_XXS      | WT 10m  |             2.24 | 14.091782 ±   0.098396 | 7.860148 ±   0.070752  | 0.812022 ±   0.002741 | -14.363 ± 0.065 % | 28.576 ± 0.084 % |
| iq1_M        | WT 10m  |             2.01 | 25.493722 ±   0.177903 | 19.262089 ±   0.152396 | 1.393084 ±   0.003529 | -24.672 ± 0.077 % | 38.287 ± 0.084 % |
| iq1_S        | WT 1m   |             1.88 | 58.097760 ±   0.438604 | 51.866126 ±   0.416604 | 2.211278 ±   0.004688 | -32.471 ± 0.087 % | 46.418 ± 0.085 % |
| iq1_S        | WT 1k   |             1.88 | 58.267851 ±   0.446208 | 52.036218 ±   0.424373 | 2.214858 ±   0.004778 | -31.880 ± 0.089 % | 46.330 ± 0.086 % |
| iq1_S        | WT 100k |             1.88 | 58.581498 ±   0.453145 | 52.349864 ±   0.431360 | 2.220834 ±   0.004818 | -32.261 ± 0.089 % | 46.002 ± 0.086 % |
| iq1_S        | WT 10m  |             1.88 | 60.694593 ±   0.471290 | 54.462959 ±   0.449644 | 2.254554 ±   0.004868 | -31.973 ± 0.088 % | 46.271 ± 0.086 % |
| iq1_S        | WT 10k  |             1.88 | 63.221324 ±   0.493077 | 56.989691 ±   0.471423 | 2.293527 ±   0.004885 | -32.261 ± 0.089 % | 46.562 ± 0.086 % |

Wikitext 토큰을 더 많이 사용하여 중요 행렬을 생성하는 것이 일관된 개선을 가져오지 않는 것 같습니다.
K-quants는 KL 분산이 시사하는 것보다 평균 Δp에서 전통적인 quants보다 더 나은 점수를 얻습니다.

## LLaMA 2 vs. LLaMA 3 Quantization 비교

| Revision | f364eb6f           |
|:---------|:-------------------|
| Backend  | CUDA               |
| CPU      | AMD Epyc 7742      |
| GPU      | 1x NVIDIA RTX 4090 |

| Metric          |          L2 7b q2_K |          L3 8b q2_K |        L2 7b q4_K_M |        L3 8b q4_K_M |          L2 7b q6_K |          L3 8b q6_K |          L2 7b q8_0 |          L3 8b q8_0 |
|-----------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|
| Mean PPL        | 5.794552 ± 0.032298 | 9.751568 ± 0.063312 | 5.877078 ± 0.032781 | 6.407115 ± 0.039119 | 5.808494 ± 0.032425 | 6.253382 ± 0.038078 | 5.798542 ± 0.032366 | 6.234284 ± 0.037878 |
| Mean PPL ratio  | 1.107955 ± 0.001427 | 1.564849 ± 0.004525 | 1.014242 ± 0.000432 | 1.028160 ± 0.000723 | 1.002406 ± 0.000191 | 1.003490 ± 0.000296 | 1.000689 ± 0.000107 | 1.000425 ± 0.000161 |
| Mean ΔPPL       | 0.625552 ± 0.008725 | 3.519934 ± 0.033863 | 0.082526 ± 0.002530 | 0.175482 ± 0.004620 | 0.013941 ± 0.001110 | 0.021748 ± 0.001852 | 0.003990 ± 0.000624 | 0.002650 ± 0.001006 |
| PPL correlation |              97.36% |              89.62% |              99.71% |              99.34% |              99.94% |              99.88% |              99.98% |              99.96% |
| Mean KLD        | 0.108903 ± 0.000645 | 0.445132 ± 0.001835 | 0.012686 ± 0.000079 | 0.031273 ± 0.000238 | 0.002098 ± 0.000014 | 0.005452 ± 0.000035 | 0.000369 ± 0.000007 | 0.001355 ± 0.000006 |
| Mean Δp         |    -2.710 ± 0.023 % |    -9.123 ± 0.051 % |    -0.416 ± 0.008 % |    -0.596 ± 0.014 % |    -0.035 ± 0.003 % |    -0.007 ± 0.006 % |    -0.005 ± 0.002 % |    -0.019 ± 0.003 % |
| Maximum Δp      |             85.136% |             94.268% |             45.209% |             95.054% |             23.593% |             53.601% |             43.925% |             28.734% |
| 99.9% Δp        |             37.184% |             50.003% |             17.461% |             27.084% |              7.798% |             13.613% |              3.387% |              6.402% |
| 99.0% Δp        |             18.131% |             25.875% |              7.798% |             12.084% |              3.838% |              6.407% |              1.867% |              3.544% |
| Median Δp       |             -0.391% |             -2.476% |             -0.026% |             -0.024% |             -0.001% |              0.000% |             -0.000% |             -0.000% |
| 1.0% Δp         |            -39.762% |            -87.173% |            -11.433% |            -19.567% |             -4.222% |             -6.767% |             -1.862% |             -3.698% |
| 0.1% Δp         |            -79.002% |            -98.897% |            -26.433% |            -56.054% |             -9.091% |            -16.584% |             -3.252% |             -6.579% |
| Minimum Δp      |            -99.915% |            -99.965% |            -83.383% |            -98.699% |            -43.142% |            -68.487% |             -9.343% |            -24.301% |
| RMS Δp          |     9.762 ± 0.053 % |    21.421 ± 0.079 % |     3.252 ± 0.024 % |     5.519 ± 0.050 % |     1.339 ± 0.010 % |     2.295 ± 0.019 % |     0.618 ± 0.011 % |     1.198 ± 0.007 % |
| Same top p      |    85.584 ± 0.086 % |    71.138 ± 0.119 % |    94.665 ± 0.055 % |    91.901 ± 0.072 % |    97.520 ± 0.038 % |    96.031 ± 0.051 % |    98.846 ± 0.026 % |    97.674 ± 0.040 % |

## LLaMA 3 BF16 vs. FP16 비교

| Revision | 83330d8c      |
|:---------|:--------------|
| Backend  | CPU           |
| CPU      | AMD Epyc 7742 |
| GPU      | N/A           |

결과는 `--kl-divergence-base`로 LLaMA 3 8b BF16을 사용하고, 비교를 위해 `--model`로 LLaMA 3 8b FP16을 사용하여 계산되었습니다.

| Metric                         |                    Value |
|--------------------------------|--------------------------|
| Mean PPL(Q)                    |      6.227711 ± 0.037833 |
| Mean PPL(base)                 |      6.225194 ± 0.037771 |
| Cor(ln(PPL(Q)), ln(PPL(base))) |                  99.990% |
| Mean ln(PPL(Q)/PPL(base))      |      0.000404 ± 0.000086 |
| Mean PPL(Q)/PPL(base)          |      1.000404 ± 0.000086 |
| Mean PPL(Q)-PPL(base)          |      0.002517 ± 0.000536 |
| Mean    KLD                    |  0.00002515 ± 0.00000020 |
| Maximum KLD                    |                 0.012206 |
| 99.9%   KLD                    |                 0.000799 |
| 99.0%   KLD                    |                 0.000222 |
| 99.0%   KLD                    |                 0.000222 |
| Median  KLD                    |                 0.000013 |
| 10.0%   KLD                    |                -0.000002 |
| 5.0%   KLD                     |                -0.000008 |
| 1.0%   KLD                     |                -0.000023 |
| Minimum KLD                    |                -0.000059 |
| Mean    Δp                     | -0.0000745 ± 0.0003952 % |
| Maximum Δp                     |                   4.186% |
| 99.9%   Δp                     |                   1.049% |
| 99.0%   Δp                     |                   0.439% |
| 95.0%   Δp                     |                   0.207% |
| 90.0%   Δp                     |                   0.125% |
| 75.0%   Δp                     |                   0.029% |
| Median  Δp                     |                   0.000% |
| 25.0%   Δp                     |                  -0.030% |
| 10.0%   Δp                     |                  -0.126% |
| 5.0%   Δp                      |                  -0.207% |
| 1.0%   Δp                      |                  -0.434% |
| 0.1%   Δp                      |                  -1.016% |
| Minimum Δp                     |                  -4.672% |
| RMS Δp                         |          0.150 ± 0.001 % |
| Same top p                     |         99.739 ± 0.013 % |

## 이전 숫자

<details>
<summary>Llama 2 70B 점수표</summary>

| Quantization | Model size (GiB) | Perplexity | Delta to fp16 |
|--------------|------------------|------------|---------------|
| Q4_0         | 36.20            | 3.5550     | 3.61%         |
| Q4_1         | 40.20            | 3.5125     | 2.37%         |
| Q5_0         | 44.20            | 3.4744     | 1.26%         |
| Q2_K         | 27.27            | 3.7339     | 8.82%         |
| Q3_K_S       | 27.86            | 3.7019     | 7.89%         |
| Q3_K_M       | 30.83            | 3.5932     | 4.72%         |
| Q3_K_L       | 33.67            | 3.5617     | 3.80%         |
| Q4_K_S       | 36.39            | 3.4852     | 1.57%         |
| Q4_K_M       | 38.54            | 3.4725     | 1.20%         |
| Q5_K_S       | 44.20            | 3.4483     | 0.50%         |
| Q5_K_M       | 45.41            | 3.4451     | 0.40%         |
| Q6_K         | 52.70            | 3.4367     | 0.16%         |
| fp16         | 128.5            | 3.4313     | -             |

</details>
